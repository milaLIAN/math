# 线性回归（二）

1. 根据梯度下降的算法：初始化模型参数 𝑤 和 𝑏 的值，在负梯度的方向上更新参数(批量梯度下降、小批量随机梯度下降或者随机梯度下降均可)，并不断迭代这一步骤，更新公式。完成代码以及注释如下所示：

```python
# Your code here
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#读取数据集
#train_frame = pd.read_csv('train.csv')
#test_frame = pd.read_csv('test.csv')
data = pd.read_csv("test.csv")
x = data["x"].values  # 提取特征列x并转换为NumPy数组
y = data["y"].values  # 提取目标列y并转换为NumPy数组

#使用均方误差作为损失函数
def loss_function(w, b, x, y):
    y_pred = w * x + b
    return 0.5 * np.sum((y_pred - y) ** 2)

#初始化模型参数
w=0.0
b=0.0

#定义学习率与迭代次数
learning_rate = 0.01
num_iterations = 16700

#使用梯度下降法训练模型，不断更新参数w,b以此减少损失函数的值
for i in range(num_iterations):
    y_pred = w * x + b
    dw = np.dot(x, (y_pred - y)) / len(x)
    db = np.sum(y_pred - y) / len(x)
    w -= learning_rate * dw
    b -= learning_rate * db

    if i % 100 == 0:
        loss = loss_function(w, b, x, y)
        print(f"Iteration {i}\tLoss = {loss}   \tw = {w}    \tb = {b}")

#可视化结果
plt.scatter(x, y, label='Data')
plt.plot(x, w * x + b, color='red', label='Linear Regression')
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.show()

```

    Iteration 0	Loss = 1937.1179284659997   	w = 0.18865601734763573    	b = 0.09909732280567302
    Iteration 100	Loss = 27.054048288877937   	w = 4.0991455171628495    	b = 2.5921569320029936
    Iteration 200	Loss = 19.59814446412488   	w = 3.9593414426247437    	b = 2.9959558242299056
    Iteration 300	Loss = 14.608274267091842   	w = 3.804036804735346    	b = 3.312801790305524
    Iteration 400	Loss = 11.152805991117773   	w = 3.674350149107012    	b = 3.576250194811072
    Iteration 500	Loss = 8.759890049471348   	w = 3.5664241138447363    	b = 3.7954806225159836
    Iteration 600	Loss = 7.10279287662117   	w = 3.476611616004925    	b = 3.977916827171868
    Iteration 700	Loss = 5.955251087761097   	w = 3.401872656804518    	b = 4.129734138024997
    Iteration 800	Loss = 5.160577016285121   	w = 3.339677393262987    	b = 4.256071429288209
    Iteration 900	Loss = 4.610264264273235   	w = 3.287920573956051    	b = 4.361205099977741
    Iteration 1000	Loss = 4.229172020424908   	w = 3.2448502790134564    	b = 4.44869382640318
    Iteration 1100	Loss = 3.9652651823761484   	w = 3.209008619280832    	b = 4.521499020032822
    Iteration 1200	Loss = 3.782509379042058   	w = 3.1791823881370376    	b = 4.582085068498926
    Iteration 1300	Loss = 3.6559507650279963   	w = 3.15436199558604    	b = 4.632502752811523
    Iteration 1400	Loss = 3.568308772330871   	w = 3.1337072945808893    	b = 4.67445866437534
    Iteration 1500	Loss = 3.507616586786962   	w = 3.1165191428328614    	b = 4.709372971510837
    Iteration 1600	Loss = 3.4655871751688734   	w = 3.102215737499634    	b = 4.738427490822091
    Iteration 1700	Loss = 3.436481756981473   	w = 3.0903129217038594    	b = 4.762605690582202
    Iteration 1800	Loss = 3.416326219817141   	w = 3.080407796276209    	b = 4.782725980213474
    Iteration 1900	Loss = 3.402368486045133   	w = 3.0721650819957533    	b = 4.7994694126800255
    Iteration 2000	Loss = 3.392702738590132   	w = 3.0653057707022717    	b = 4.813402737492924
    Iteration 2100	Loss = 3.3860091968311457   	w = 3.0595976811312857    	b = 4.824997584650421
    Iteration 2200	Loss = 3.3813739110978944   	w = 3.054847599795664    	b = 4.83464642887182
    Iteration 2300	Loss = 3.3781639701424897   	w = 3.0508947408899947    	b = 4.8426758744991965
    Iteration 2400	Loss = 3.375941082163168   	w = 3.0476053038416295    	b = 4.849357710748558
    Iteration 2500	Loss = 3.374401729559094   	w = 3.0448679442865934    	b = 4.854918111520315
    Iteration 2600	Loss = 3.3733357260259886   	w = 3.0425900051671104    	b = 4.859545291174039
    Iteration 2700	Loss = 3.3725975172948663   	w = 3.0406943803769617    	b = 4.863395875408291
    Iteration 2800	Loss = 3.3720863068590843   	w = 3.0391169047919964    	b = 4.866600202893856
    Iteration 2900	Loss = 3.3717322930163407   	w = 3.0378041823407496    	b = 4.8692667371157095
    Iteration 3000	Loss = 3.3714871380036877   	w = 3.036711778597434    	b = 4.871485737760408
    Iteration 3100	Loss = 3.371317367862031   	w = 3.0358027167182287    	b = 4.8733323159219815
    Iteration 3200	Loss = 3.3711998018343756   	w = 3.03504622580973    	b = 4.874868976542142
    Iteration 3300	Loss = 3.371118387218567   	w = 3.0344166993630384    	b = 4.876147734144001
    Iteration 3400	Loss = 3.371062007499022   	w = 3.0338928284973963    	b = 4.877211873475008
    Iteration 3500	Loss = 3.3710229644753027   	w = 3.033456880674539    	b = 4.878097414655085
    Iteration 3600	Loss = 3.370995927134334   	w = 3.0330940994688613    	b = 4.878834332424004
    Iteration 3700	Loss = 3.370977203743138   	w = 3.032792205076251    	b = 4.879447570758214
    Iteration 3800	Loss = 3.3709642377721716   	w = 3.0325409786541986    	b = 4.879957887201114
    Iteration 3900	Loss = 3.370955258821658   	w = 3.032331916423557    	b = 4.880382555486458
    Iteration 4000	Loss = 3.370949040887607   	w = 3.0321579418235785    	b = 4.880735950238055
    Iteration 4100	Loss = 3.370944734960785   	w = 3.0320131659769585    	b = 4.88103003353734
    Iteration 4200	Loss = 3.3709417531012713   	w = 3.03189268835685    	b = 4.881274759828631
    Iteration 4300	Loss = 3.3709396881599636   	w = 3.0317924309086224    	b = 4.881478412867732
    Iteration 4400	Loss = 3.37093825818562   	w = 3.0317090000115328    	b = 4.8816478861193255
    Iteration 4500	Loss = 3.370937267926669   	w = 3.031639571607833    	b = 4.88178891609429
    Iteration 4600	Loss = 3.3709365821711526   	w = 3.0315817956110696    	b = 4.8819062765252195
    Iteration 4700	Loss = 3.3709361072846242   	w = 3.031533716357852    	b = 4.882003939952811
    Iteration 4800	Loss = 3.3709357784251353   	w = 3.0314937064104783    	b = 4.882085212192643
    Iteration 4900	Loss = 3.370935550689544   	w = 3.0314604114696926    	b = 4.882152844233926
    Iteration 5000	Loss = 3.3709353929823713   	w = 3.03143270453293    	b = 4.88220912535793
    Iteration 5100	Loss = 3.370935283769964   	w = 3.0314096477463206    	b = 4.8822559606280045
    Iteration 5200	Loss = 3.3709352081402395   	w = 3.031390460659227    	b = 4.882294935374192
    Iteration 5300	Loss = 3.370935155766566   	w = 3.0313744938067178    	b = 4.882327368855159
    Iteration 5400	Loss = 3.3709351194977324   	w = 3.0313612067257836    	b = 4.882354358913858
    Iteration 5500	Loss = 3.370935094381521   	w = 3.0313501496611823    	b = 4.882376819138507
    Iteration 5600	Loss = 3.3709350769885136   	w = 3.031340948341628    	b = 4.882395509786673
    Iteration 5700	Loss = 3.370935064943839   	w = 3.031333291311069    	b = 4.882411063519302
    Iteration 5800	Loss = 3.3709350566028835   	w = 3.0313269193861916    	b = 4.8824240068156755
    Iteration 5900	Loss = 3.3709350508267626   	w = 3.031321616883303    	b = 4.8824347777942645
    Iteration 6000	Loss = 3.3709350468267933   	w = 3.031317204317646    	b = 4.882443741042602
    Iteration 6100	Loss = 3.370935044056808   	w = 3.0313135323280074    	b = 4.88245119995825
    Iteration 6200	Loss = 3.370935042138596   	w = 3.031310476620976    	b = 4.88245740701852
    Iteration 6300	Loss = 3.3709350408102248   	w = 3.031307933763727    	b = 4.882462572326596
    Iteration 6400	Loss = 3.3709350398903273   	w = 3.0313058176829233    	b = 4.8824668707233165
    Iteration 6500	Loss = 3.3709350392532995   	w = 3.0313040567512113    	b = 4.8824704477053835
    Iteration 6600	Loss = 3.3709350388121564   	w = 3.031302591362707    	b = 4.88247342435029
    Iteration 6700	Loss = 3.3709350385066594   	w = 3.0313013719153763    	b = 4.88247590141468
    Iteration 6800	Loss = 3.3709350382951087   	w = 3.0313003571320705    	b = 4.882477962744865
    Iteration 6900	Loss = 3.3709350381486063   	w = 3.0312995126633244    	b = 4.88247967811494
    Iteration 7000	Loss = 3.370935038047154   	w = 3.031298809924662    	b = 4.882481105588579
    Iteration 7100	Loss = 3.3709350379768965   	w = 3.0312982251290372    	b = 4.882482293484423
    Iteration 7200	Loss = 3.3709350379282483   	w = 3.0312977384816673    	b = 4.882483282011636
    Iteration 7300	Loss = 3.3709350378945553   	w = 3.031297333509994    	b = 4.882484104630936
    Iteration 7400	Loss = 3.370935037871219   	w = 3.031296996506104    	b = 4.882484789187223
    Iteration 7500	Loss = 3.370935037855067   	w = 3.0312967160627324    	b = 4.8824853588520805
    Iteration 7600	Loss = 3.3709350378438767   	w = 3.0312964826871323    	b = 4.882485832908133
    Iteration 7700	Loss = 3.37093503783613   	w = 3.031296288479759    	b = 4.882486227401734
    Iteration 7800	Loss = 3.3709350378307654   	w = 3.0312961268668746    	b = 4.882486555686124
    Iteration 7900	Loss = 3.370935037827044   	w = 3.0312959923780336    	b = 4.882486828873419
    Iteration 8000	Loss = 3.3709350378244753   	w = 3.031295880460917    	b = 4.882487056210752
    Iteration 8100	Loss = 3.3709350378226937   	w = 3.0312957873272333    	b = 4.882487245393283
    Iteration 8200	Loss = 3.3709350378214635   	w = 3.0312957098244966    	b = 4.882487402824657
    Iteration 8300	Loss = 3.370935037820603   	w = 3.0312956453293087    	b = 4.882487533833787
    Iteration 8400	Loss = 3.370935037820013   	w = 3.0312955916585684    	b = 4.882487642855207
    Iteration 8500	Loss = 3.370935037819602   	w = 3.0312955469955747    	b = 4.882487733579185
    Iteration 8600	Loss = 3.3709350378193177   	w = 3.0312955098285226    	b = 4.8824878090766495
    Iteration 8700	Loss = 3.3709350378191245   	w = 3.0312954788993434    	b = 4.882487871903116
    Iteration 8800	Loss = 3.370935037818983   	w = 3.0312954531611154    	b = 4.882487924185201
    Iteration 8900	Loss = 3.370935037818893   	w = 3.0312954317426217    	b = 4.882487967692605
    Iteration 9000	Loss = 3.3709350378188256   	w = 3.0312954139188664    	b = 4.882488003898013
    Iteration 9100	Loss = 3.370935037818783   	w = 3.031295399086535    	b = 4.88248803402694
    Iteration 9200	Loss = 3.370935037818752   	w = 3.0312953867435652    	b = 4.882488059099227
    Iteration 9300	Loss = 3.3709350378187284   	w = 3.031295376472157    	b = 4.882488079963548
    Iteration 9400	Loss = 3.3709350378187146   	w = 3.031295367924634    	b = 4.882488097326136
    Iteration 9500	Loss = 3.370935037818704   	w = 3.0312953608116717    	b = 4.8824881117747
    Iteration 9600	Loss = 3.370935037818695   	w = 3.0312953548925026    	b = 4.882488123798315
    Iteration 9700	Loss = 3.370935037818689   	w = 3.0312953499667676    	b = 4.882488133803968
    Iteration 9800	Loss = 3.3709350378186898   	w = 3.0312953458677336    	b = 4.882488142130336
    Iteration 9900	Loss = 3.3709350378186826   	w = 3.031295342456656    	b = 4.882488149059261
    Iteration 10000	Loss = 3.370935037818684   	w = 3.031295339618071    	b = 4.882488154825283
    Iteration 10100	Loss = 3.3709350378186818   	w = 3.0312953372558957    	b = 4.8824881596235725
    Iteration 10200	Loss = 3.3709350378186835   	w = 3.031295335290173    	b = 4.882488163616545
    Iteration 10300	Loss = 3.3709350378186818   	w = 3.0312953336543638    	b = 4.882488166939364
    Iteration 10400	Loss = 3.37093503781868   	w = 3.031295332293098    	b = 4.882488169704507
    Iteration 10500	Loss = 3.370935037818679   	w = 3.0312953311602984    	b = 4.882488172005562
    Iteration 10600	Loss = 3.3709350378186773   	w = 3.0312953302176204    	b = 4.882488173920427
    Iteration 10700	Loss = 3.370935037818681   	w = 3.031295329433156    	b = 4.882488175513908
    Iteration 10800	Loss = 3.3709350378186773   	w = 3.0312953287803497    	b = 4.882488176839952
    Iteration 10900	Loss = 3.370935037818678   	w = 3.031295328237108    	b = 4.882488177943443
    Iteration 11000	Loss = 3.3709350378186795   	w = 3.031295327785038    	b = 4.882488178861731
    Iteration 11100	Loss = 3.370935037818679   	w = 3.031295327408841    	b = 4.8824881796259
    Iteration 11200	Loss = 3.370935037818679   	w = 3.0312953270957825    	b = 4.882488180261816
    Iteration 11300	Loss = 3.370935037818679   	w = 3.0312953268352683    	b = 4.882488180791003
    Iteration 11400	Loss = 3.3709350378186773   	w = 3.0312953266184732    	b = 4.882488181231376
    Iteration 11500	Loss = 3.3709350378186818   	w = 3.031295326438064    	b = 4.88248818159784
    Iteration 11600	Loss = 3.370935037818678   	w = 3.0312953262879354    	b = 4.882488181902799
    Iteration 11700	Loss = 3.3709350378186826   	w = 3.0312953261630007    	b = 4.882488182156577
    Iteration 11800	Loss = 3.37093503781868   	w = 3.031295326059038    	b = 4.882488182367759
    Iteration 11900	Loss = 3.370935037818677   	w = 3.031295325972522    	b = 4.882488182543496
    Iteration 12000	Loss = 3.370935037818677   	w = 3.031295325900525    	b = 4.882488182689745
    Iteration 12100	Loss = 3.3709350378186764   	w = 3.0312953258406123    	b = 4.882488182811447
    Iteration 12200	Loss = 3.3709350378186826   	w = 3.031295325790752    	b = 4.882488182912728
    Iteration 12300	Loss = 3.370935037818679   	w = 3.031295325749263    	b = 4.882488182997004
    Iteration 12400	Loss = 3.370935037818679   	w = 3.031295325714738    	b = 4.882488183067135
    Iteration 12500	Loss = 3.370935037818678   	w = 3.031295325686007    	b = 4.882488183125496
    Iteration 12600	Loss = 3.370935037818681   	w = 3.0312953256620965    	b = 4.882488183174065
    Iteration 12700	Loss = 3.3709350378186795   	w = 3.031295325642201    	b = 4.882488183214479
    Iteration 12800	Loss = 3.3709350378186844   	w = 3.0312953256256434    	b = 4.882488183248111
    Iteration 12900	Loss = 3.3709350378186786   	w = 3.031295325611866    	b = 4.882488183276098
    Iteration 13000	Loss = 3.370935037818683   	w = 3.0312953256004005    	b = 4.8824881832993885
    Iteration 13100	Loss = 3.3709350378186818   	w = 3.0312953255908583    	b = 4.882488183318771
    Iteration 13200	Loss = 3.3709350378186773   	w = 3.0312953255829185    	b = 4.8824881833349
    Iteration 13300	Loss = 3.3709350378186764   	w = 3.0312953255763113    	b = 4.882488183348321
    Iteration 13400	Loss = 3.3709350378186826   	w = 3.031295325570812    	b = 4.882488183359491
    Iteration 13500	Loss = 3.3709350378186813   	w = 3.031295325566237    	b = 4.8824881833687845
    Iteration 13600	Loss = 3.3709350378186773   	w = 3.03129532556243    	b = 4.882488183376518
    Iteration 13700	Loss = 3.3709350378186747   	w = 3.031295325559261    	b = 4.8824881833829545
    Iteration 13800	Loss = 3.370935037818679   	w = 3.0312953255566244    	b = 4.88248818338831
    Iteration 13900	Loss = 3.370935037818679   	w = 3.031295325554431    	b = 4.882488183392767
    Iteration 14000	Loss = 3.3709350378186795   	w = 3.0312953255526054    	b = 4.882488183396473
    Iteration 14100	Loss = 3.3709350378186813   	w = 3.031295325551088    	b = 4.882488183399558
    Iteration 14200	Loss = 3.370935037818679   	w = 3.0312953255498245    	b = 4.882488183402124
    Iteration 14300	Loss = 3.370935037818678   	w = 3.031295325548773    	b = 4.882488183404258
    Iteration 14400	Loss = 3.3709350378186818   	w = 3.0312953255478994    	b = 4.882488183406036
    Iteration 14500	Loss = 3.370935037818679   	w = 3.0312953255471715    	b = 4.882488183407514
    Iteration 14600	Loss = 3.370935037818678   	w = 3.0312953255465644    	b = 4.8824881834087455
    Iteration 14700	Loss = 3.3709350378186773   	w = 3.0312953255460604    	b = 4.882488183409772
    Iteration 14800	Loss = 3.370935037818681   	w = 3.03129532554564    	b = 4.88248818341062
    Iteration 14900	Loss = 3.37093503781868   	w = 3.0312953255452917    	b = 4.882488183411331
    Iteration 15000	Loss = 3.3709350378186778   	w = 3.0312953255450013    	b = 4.882488183411919
    Iteration 15100	Loss = 3.3709350378186804   	w = 3.0312953255447592    	b = 4.88248818341241
    Iteration 15200	Loss = 3.3709350378186804   	w = 3.0312953255445576    	b = 4.88248818341282
    Iteration 15300	Loss = 3.370935037818679   	w = 3.0312953255443844    	b = 4.882488183413174
    Iteration 15400	Loss = 3.370935037818679   	w = 3.031295325544251    	b = 4.88248818341344
    Iteration 15500	Loss = 3.3709350378186826   	w = 3.03129532554413    	b = 4.88248818341369
    Iteration 15600	Loss = 3.37093503781868   	w = 3.031295325544041    	b = 4.882488183413868
    Iteration 15700	Loss = 3.37093503781868   	w = 3.031295325543953    	b = 4.882488183414045
    Iteration 15800	Loss = 3.37093503781868   	w = 3.0312953255438875    	b = 4.882488183414182
    Iteration 15900	Loss = 3.3709350378186813   	w = 3.031295325543843    	b = 4.882488183414271
    Iteration 16000	Loss = 3.3709350378186804   	w = 3.0312953255437987    	b = 4.88248818341436
    Iteration 16100	Loss = 3.3709350378186773   	w = 3.0312953255437542    	b = 4.8824881834144485
    Iteration 16200	Loss = 3.3709350378186764   	w = 3.031295325543711    	b = 4.882488183414537
    Iteration 16300	Loss = 3.370935037818682   	w = 3.0312953255436703    	b = 4.882488183414624
    Iteration 16400	Loss = 3.37093503781868   	w = 3.03129532554367    	b = 4.882488183414625
    Iteration 16500	Loss = 3.37093503781868   	w = 3.03129532554367    	b = 4.882488183414625
    Iteration 16600	Loss = 3.37093503781868   	w = 3.03129532554367    	b = 4.882488183414625

![这是图片](resource/output_2_1.png "Magic Gardens")

```python
不断增加迭代次数，发现当数据到达16400的时候，w、b的值就不会发生改变了，此时的Loss的值最小，w=3.03129532554367    b=4.882488183414625
```

```python
将test.csv中的数据导入，进行可视化之后的结果，如上述所示。
```

2.根据三元线性回归模型的算法，代码及注释如下所示：

```python
# Your code here


import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 读取训练数据集
train_data = pd.read_csv('train2.csv')
X_train = train_data[['x1', 'x2', 'x3']].values
y_train = train_data['y'].values

# 读取测试数据集
test_data = pd.read_csv('test2.csv')
X_test = test_data[['x1', 'x2', 'x3']].values
y_test = test_data['y'].values

# 创建并训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 提取模型参数
w0 = model.intercept_
w1, w2, w3 = model.coef_

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)

print("w1=",w1,"\nw2=",w2,"\nw3=",w3,"\nmse=",mse)
```

    w1= 1.0072300078584722
    w2= 2.0033937115307494
    w3= 3.0102562412206275
    mse= 0.1653769112895223

```python
由打印的结果可以得出，mse= 0.1653769112895223
```
